
[2025-08-18 03:09:42]
‚úÖ requests already installed.
----------------------------------------

[2025-08-18 03:09:42]
üì¶ Installing beautifulsoup4 ...
----------------------------------------

[2025-08-18 03:09:47]
üìú Executing Code:
import requests
from bs4 import BeautifulSoup

url = "https://en.wikipedia.org/wiki/List_of_highest-grossing_films"
response = requests.get(url)

if response.status_code == 200:
    soup = BeautifulSoup(response.content, "html.parser")
    # Find the main table, which is the first wikitable
    table = soup.find("table", {"class": "wikitable"})
    if table:
        headers = [header.get_text(strip=True) for header in table.find_all("th")]
        summary = f"Found a table with the following headers: {', '.join(headers)}. The table lists the highest-grossing films with details on rank, peak, title, worldwide gross, year, and references."
        with open(
            "uploads/ac1e3888-31ad-4afd-b95d-e91192777d31/metadata.txt", "a"
        ) as f:
            f.write(summary + "\n")
        print("Successfully scraped basic info and saved to metadata.txt")
    else:
        print("Could not find the main table on the page.")
else:
    print(f"Failed to retrieve the page. Status code: {response.status_code}")

----------------------------------------

[2025-08-18 03:09:54]
‚úÖ Code executed successfully:
Successfully scraped basic info and saved to metadata.txt

----------------------------------------

[2025-08-18 03:10:06]
‚úÖ requests already installed.
----------------------------------------

[2025-08-18 03:10:07]
üì¶ Installing beautifulsoup4 ...
----------------------------------------

[2025-08-18 03:10:08]
‚úÖ pandas already installed.
----------------------------------------

[2025-08-18 03:10:08]
üìú Executing Code:
import requests
from bs4 import BeautifulSoup
import pandas as pd
import re

url = "https://en.wikipedia.org/wiki/List_of_highest-grossing_films"
response = requests.get(url)

soup = BeautifulSoup(response.content, "html.parser")
table = soup.find("table", {"class": "wikitable"})

headers = [header.get_text(strip=True) for header in table.find_all("th")[:6]]

film_data = []
for row in table.find("tbody").find_all("tr")[1:]:  # Skip the header row
    cells = row.find_all(["td", "th"])
    if len(cells) >= 5:
        rank = cells[0].get_text(strip=True)
        peak = cells[1].get_text(strip=True)
        title = cells[2].get_text(strip=True)
        gross = cells[3].get_text(strip=True)
        year = cells[4].get_text(strip=True)

        # Clean and prepare data
        try:
            gross_cleaned = int(re.sub(r"[$,]", "", gross))
            year_cleaned = (
                int(re.search(r"\d{4}", year).group())
                if re.search(r"\d{4}", year)
                else None
            )
            rank_cleaned = int(rank)
            peak_cleaned = int(peak)

            film_data.append(
                {
                    "Rank": rank_cleaned,
                    "Peak": peak_cleaned,
                    "Title": title,
                    "Worldwide_gross": gross_cleaned,
                    "Year": year_cleaned,
                }
            )
        except (ValueError, AttributeError) as e:
            # Skip rows that cannot be parsed correctly
            continue

df = pd.DataFrame(film_data)
df.to_csv(
    "uploads/ac1e3888-31ad-4afd-b95d-e91192777d31/highest_grossing_films.csv",
    index=False,
)

with open("uploads/ac1e3888-31ad-4afd-b95d-e91192777d31/metadata.txt", "a") as f:
    f.write(
        "Successfully downloaded and saved the film data to highest_grossing_films.csv.\n"
    )

print("Data downloaded and saved.")

----------------------------------------

[2025-08-18 03:10:14]
‚úÖ Code executed successfully:
Data downloaded and saved.

----------------------------------------

[2025-08-18 03:10:36]
‚úÖ pandas already installed.
----------------------------------------

[2025-08-18 03:10:36]
‚úÖ numpy already installed.
----------------------------------------

[2025-08-18 03:10:36]
‚úÖ matplotlib already installed.
----------------------------------------

[2025-08-18 03:10:36]
üìú Executing Code:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import io
import base64
import json

# Load the dataset
df = pd.read_csv(
    "uploads/ac1e3888-31ad-4afd-b95d-e91192777d31/highest_grossing_films.csv"
)

# Ensure numeric types for calculations
df["Worldwide_gross"] = pd.to_numeric(df["Worldwide_gross"], errors="coerce")
df["Year"] = pd.to_numeric(df["Year"], errors="coerce")
df["Rank"] = pd.to_numeric(df["Rank"], errors="coerce")
df["Peak"] = pd.to_numeric(df["Peak"], errors="coerce")
df.dropna(inplace=True)

# 1. How many $2 bn movies were released before 2000?
movies_2bn_before_2000 = df[
    (df["Worldwide_gross"] >= 2_000_000_000) & (df["Year"] < 2000)
].shape[0]

# 2. Which is the earliest film that grossed over $1.5 bn?
earliest_film_1_5bn = (
    df[df["Worldwide_gross"] >= 1_500_000_000].sort_values("Year").iloc[0]["Title"]
)

# 3. What's the correlation between the Rank and Peak?
correlation = df["Rank"].corr(df["Peak"])

# 4. Draw a scatterplot of Rank and Peak
plt.figure(figsize=(10, 6))
plt.scatter(df["Rank"], df["Peak"], alpha=0.7, label="Films")

# Calculate and plot regression line
z = np.polyfit(df["Rank"], df["Peak"], 1)
p = np.poly1d(z)
plt.plot(
    df["Rank"], p(df["Rank"]), "r--", label=f"Regression Line (corr={correlation:.2f})"
)

plt.title("Rank vs. Peak Position of Highest-Grossing Films")
plt.xlabel("Rank")
plt.ylabel("Peak")
plt.grid(True)
plt.legend()
plt.tight_layout()

# Save plot to a buffer
buf = io.BytesIO()
plt.savefig(buf, format="png", dpi=72)
plt.close()
buf.seek(0)

# Encode image to base64 and create data URI
image_base64 = base64.b64encode(buf.read()).decode("utf-8")
data_uri = f"data:image/png;base64,{image_base64}"

# Combine answers into a list
final_answers = [
    str(movies_2bn_before_2000),
    earliest_film_1_5bn,
    str(correlation),
    data_uri,
]

# Save the final answers to a JSON file
with open("uploads/ac1e3888-31ad-4afd-b95d-e91192777d31/result.json", "w") as f:
    json.dump(final_answers, f)

print("Final answers have been computed and saved to result.json.")

----------------------------------------

[2025-08-18 03:10:38]
‚úÖ Code executed successfully:
Final answers have been computed and saved to result.json.

----------------------------------------

[2025-08-18 03:11:15]
‚úÖ pandas already installed.
----------------------------------------

[2025-08-18 03:11:15]
‚úÖ numpy already installed.
----------------------------------------

[2025-08-18 03:11:15]
‚úÖ matplotlib already installed.
----------------------------------------

[2025-08-18 03:11:15]
‚úÖ lxml already installed.
----------------------------------------

[2025-08-18 03:11:15]
üìú Executing Code:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import io
import base64
import json
import re

# Step 1: Scrape data using pandas for robustness
url = "https://en.wikipedia.org/wiki/List_of_highest-grossing_films"
tables = pd.read_html(url, attrs={"class": "wikitable"})
df = tables[0]

# Step 2: Clean the DataFrame
# Select and rename necessary columns
df = df.iloc[:, [0, 1, 2, 3, 4]]  # Rank, Peak, Title, Worldwide gross, Year
df.columns = ["Rank", "Peak", "Title", "Worldwide_gross", "Year"]

# Clean 'Worldwide_gross' column
df["Worldwide_gross"] = (
    df["Worldwide_gross"].astype(str).apply(lambda x: re.sub(r"[^\d]", "", x))
)
df["Worldwide_gross"] = pd.to_numeric(df["Worldwide_gross"], errors="coerce")

# Clean 'Year' column
df["Year"] = df["Year"].astype(str).str.extract(r"(\d{4})")
df["Year"] = pd.to_numeric(df["Year"], errors="coerce")

# Clean 'Rank' and 'Peak' columns
df["Rank"] = pd.to_numeric(df["Rank"], errors="coerce")
df["Peak"] = pd.to_numeric(df["Peak"], errors="coerce")

# Drop rows where essential data could not be parsed
df.dropna(subset=["Worldwide_gross", "Year", "Rank", "Peak", "Title"], inplace=True)

# Save the cleaned data for verification
df.to_csv(
    "uploads/ac1e3888-31ad-4afd-b95d-e91192777d31/highest_grossing_films_cleaned.csv",
    index=False,
)

# Step 3: Answer the questions
# 1. How many $2 bn movies were released before 2000?
movies_2bn_before_2000 = df[
    (df["Worldwide_gross"] >= 2_000_000_000) & (df["Year"] < 2000)
].shape[0]

# 2. Which is the earliest film that grossed over $1.5 bn?
earliest_film_1_5bn_row = (
    df[df["Worldwide_gross"] >= 1_500_000_000].sort_values("Year").iloc[0]
)
earliest_film_1_5bn = earliest_film_1_5bn_row["Title"]

# 3. What's the correlation between the Rank and Peak?
correlation = df["Rank"].corr(df["Peak"])

# 4. Draw a scatterplot of Rank and Peak
plt.figure(figsize=(10, 6))
plt.scatter(df["Rank"], df["Peak"], alpha=0.7, label="Films")

# Calculate and plot regression line
z = np.polyfit(df["Rank"], df["Peak"], 1)
p = np.poly1d(z)
plt.plot(
    df["Rank"], p(df["Rank"]), "r--", label=f"Regression Line (corr={correlation:.2f})"
)

plt.title("Rank vs. Peak Position of Highest-Grossing Films")
plt.xlabel("Rank")
plt.ylabel("Peak")
plt.grid(True)
plt.legend()
plt.tight_layout()

# Save plot to a buffer and encode to base64
buf = io.BytesIO()
plt.savefig(buf, format="png", dpi=72)
plt.close()
buf.seek(0)
image_base64 = base64.b64encode(buf.read()).decode("utf-8")
data_uri = f"data:image/png;base64,{image_base64}"

# Combine answers into a JSON array
final_answers = [
    str(movies_2bn_before_2000),
    earliest_film_1_5bn,
    str(correlation),
    data_uri,
]

# Save the final answers to result.json
with open("uploads/ac1e3888-31ad-4afd-b95d-e91192777d31/result.json", "w") as f:
    json.dump(final_answers, f)

print("Corrected answers have been computed and saved to result.json.")

----------------------------------------

[2025-08-18 03:11:21]
‚ùå Execution error:
/home/felicia/TDS_project2/uploads/ac1e3888-31ad-4afd-b95d-e91192777d31/job_/script.py:32: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df.dropna(subset=['Worldwide_gross', 'Year', 'Rank', 'Peak', 'Title'], inplace=True)
Traceback (most recent call last):
  File "/home/felicia/TDS_project2/uploads/ac1e3888-31ad-4afd-b95d-e91192777d31/job_/script.py", line 42, in <module>
    earliest_film_1_5bn_row = df[df['Worldwide_gross'] >= 1_500_000_000].sort_values('Year').iloc[0]
                              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^
  File "/home/felicia/TDS_project2/venv/lib/python3.13/site-packages/pandas/core/indexing.py", line 1191, in __getitem__
    return self._getitem_axis(maybe_callable, axis=axis)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/felicia/TDS_project2/venv/lib/python3.13/site-packages/pandas/core/indexing.py", line 1752, in _getitem_axis
    self._validate_integer(key, axis)
    ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^
  File "/home/felicia/TDS_project2/venv/lib/python3.13/site-packages/pandas/core/indexing.py", line 1685, in _validate_integer
    raise IndexError("single positional indexer is out-of-bounds")
IndexError: single positional indexer is out-of-bounds

----------------------------------------

[2025-08-18 03:11:46]
üìú Executing Code:

----------------------------------------

[2025-08-18 03:11:46]
‚úÖ Code executed successfully:

----------------------------------------
